





# Core libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

# Probability and statistics
from scipy import stats
from scipy.optimize import minimize
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix
from sklearn.model_selection import train_test_split

# Bayesian modeling
import pymc as pm
import arviz as az

# Utilities
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
np.random.seed(42)

# Configure plotting
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# Check for PyMC/PyTensor warnings and provide helpful message
import pytensor
try:
    # Test if C++ compiler is available
    pytensor.config.gcc__cxxflags
    compiler_status = "C++ compiler detected - optimal performance"
except:
    compiler_status = "C++ compiler not detected - using Python fallback"
    print("\nNOTE: For optimal performance, install a C++ compiler:")
    print("  - Conda users: conda install gxx")
    print("  - See INSTALLATION.md for detailed instructions")
    print("  - Current setup will work but may be slower\n")

print("All libraries imported successfully!")
print(f"NumPy version: {np.__version__}")
print(f"Pandas version: {pd.__version__}")
print(f"PyMC version: {pm.__version__}")
print(f"Compiler status: {compiler_status}")





class TransactionDataGenerator: """Generate realistic synthetic transaction data with fraud patterns."""
    
    def __init__(self, n_users=10000, n_transactions=100000, fraud_rate=0.02):
        self.n_users = n_users
        self.n_transactions = n_transactions
        self.fraud_rate = fraud_rate
        
        # Merchant categories
        self.merchant_categories = [
            'grocery', 'gas_station', 'restaurant', 'retail', 'online',
            'pharmacy', 'entertainment', 'travel', 'utilities', 'other'
]
        
        # Geographic regions
        self.regions = ['north', 'south', 'east', 'west', 'central'
]
    
    def generate_users(self): """Generate user profiles with spending patterns."""
        users = pd.DataFrame({
            'user_id': range(self.n_users),
            'age': np.random.normal(40,
    15, self.n_users).clip(18,
    80),
            'income': np.random.lognormal(10.5,
    0.5, self.n_users),
            'region': np.random.choice(self.regions, self.n_users),
            'account_age_days': np.random.exponential(365, self.n_users).clip(1,
    3650),
            'avg_monthly_spend': np.random.lognormal(7,
    0.8, self.n_users)
})
        
        return users
    
    def generate_transactions(self, users): """Generate transaction data with normal and fraudulent patterns."""
        transactions = []
        
        # Generate timestamps over 6 months
        start_date = datetime(2024,
1,
1)
        end_date = datetime(2024,
6,
30)
        
        for i in range(self.n_transactions):
            # Select random user
            user = users.sample(1).iloc[
    0
]
            
            # Generate timestamp with daily patterns
            days_offset = np.random.uniform(0, (end_date - start_date).days)
            timestamp = start_date + timedelta(days=days_offset)
            
            # Hour of day (more activity during business hours)
            hour_weights = np.array([
    0.5,
    0.3,
    0.2,
    0.2,
    0.3,
    0.5,
    0.8,
    1.2,
    1.5,
    1.8,
    2.0,
    2.2,
    2.5,
    2.3,
    2.0,
    1.8,
    1.5,
    1.3,
    1.0,
    0.8,
    0.7,
    0.6,
    0.5,
    0.4
])
            hour = np.random.choice(24, p=hour_weights/hour_weights.sum())
            timestamp = timestamp.replace(hour=hour, minute=np.random.randint(0,
60))
            
            # Determine if fraudulent
            is_fraud = np.random.random() < self.fraud_rate
            
            if is_fraud:
                # Fraudulent transaction patterns
                amount = self._generate_fraud_amount(user)
                merchant_category = np.random.choice(['online', 'retail', 'other'
], 
                                                   p=[
    0.6,
    0.3,
    0.1
])
                # Fraud often happens in different regions
                region = np.random.choice([r for r in self.regions if r != user['region'
    ]
])
            else:
                # Normal transaction patterns
                amount = self._generate_normal_amount(user)
                merchant_category = np.random.choice(self.merchant_categories)
                region = user['region'
] if np.random.random() < 0.9 else np.random.choice(self.regions)
            
            transactions.append({
                'transaction_id': f'txn_{i: 06d
    }',
                'user_id': user['user_id'
    ],
                'timestamp': timestamp,
                'amount': amount,
                'merchant_category': merchant_category,
                'region': region,
                'is_fraud': is_fraud,
                'hour': hour,
                'day_of_week': timestamp.weekday(),
                'user_age': user['age'
    ],
                'user_income': user['income'
    ],
                'account_age_days': user['account_age_days'
    ]
})
        
        return pd.DataFrame(transactions)
    
    def _generate_normal_amount(self, user): """Generate normal transaction amounts based on user profile."""
        base_amount = user['avg_monthly_spend'
] / 30
        return np.random.lognormal(np.log(base_amount),
0.8)
    
    def _generate_fraud_amount(self, user): """Generate fraudulent transaction amounts (typically higher)."""
        if np.random.random() < 0.3:  # Small fraud
            return np.random.uniform(10,
100)
        else:  # Large fraud
            return np.random.uniform(500,
5000)

# Generate the dataset
print("Generating synthetic transaction data...")
generator = TransactionDataGenerator(n_users=5000, n_transactions=50000, fraud_rate=0.025)
users = generator.generate_users()
transactions = generator.generate_transactions(users)

print(f"Generated {len(transactions):,} transactions for {len(users):,} users")
print(f"Fraud rate: {transactions['is_fraud'].mean():.2%}")
print(f"Total transaction volume: ${transactions['amount'].sum():,.2f}")





# Basic statistics
print("Transaction Dataset Overview")
print("=" * 50)
print(transactions.describe())
print("\nFraud Distribution by Category")
print(transactions.groupby(['merchant_category', 'is_fraud'
]).size().unstack(fill_value=0))

# Create comprehensive visualization
fig = make_subplots(
    rows=2, cols=2,
    subplot_titles=('Transaction Amounts by Fraud Status', 'Fraud Rate by Hour', 
                   'Fraud Rate by Merchant Category', 'Transaction Volume Over Time'),
    specs=[
    [
        {
            "secondary_y": False
        },
        {
            "secondary_y": False
        }
    ],
    [
        {
            "secondary_y": False
        },
        {
            "secondary_y": False
        }
    ]
]
)

# Amount distribution
for fraud_status in [False, True
]:
    data = transactions[transactions['is_fraud'
    ] == fraud_status
]['amount'
]
    fig.add_trace(
        go.Histogram(x=data, name=f'Fraud: {fraud_status
}', opacity=0.7, nbinsx=50),
        row=1, col=1
    )

# Fraud rate by hour
hourly_fraud = transactions.groupby('hour')['is_fraud'
].agg(['mean', 'count'
]).reset_index()
fig.add_trace(
    go.Scatter(x=hourly_fraud['hour'
], y=hourly_fraud['mean'
], 
              mode='lines+markers', name='Fraud Rate'),
    row=1, col=2
)

# Fraud rate by category
category_fraud = transactions.groupby('merchant_category')['is_fraud'
].mean().reset_index()
fig.add_trace(
    go.Bar(x=category_fraud['merchant_category'
], y=category_fraud['is_fraud'
], 
           name='Fraud Rate by Category'),
    row=2, col=1
)

# Daily transaction volume
daily_volume = transactions.groupby(transactions['timestamp'
].dt.date).agg({
    'amount': 'sum',
    'is_fraud': 'sum'
}).reset_index()
fig.add_trace(
    go.Scatter(x=daily_volume['timestamp'
], y=daily_volume['amount'
], 
              mode='lines', name='Daily Volume'),
    row=2, col=2
)

fig.update_layout(height=800, title_text="Transaction Data Exploratory Analysis")
fig.show()

# Feature engineering for modeling
transactions['log_amount'
] = np.log1p(transactions['amount'
])
transactions['is_weekend'
] = transactions['day_of_week'
].isin([
    5,
    6
]).astype(int)
transactions['is_night'
] = transactions['hour'
].isin(list(range(22,
24)) + list(range(0,
6))).astype(int)

print("\nFeature engineering completed")
print(f"Dataset shape: {transactions.shape}")





class BayesianFraudDetector: """Bayesian fraud detection using conjugate priors and online updating."""
    
    def __init__(self):
        # Beta-Binomial model for fraud rate
        self.alpha_prior = 1  # Prior successes (frauds)
        self.beta_prior = 39  # Prior failures (normal transactions)
        
        # Current posterior parameters
        self.alpha_post = self.alpha_prior
        self.beta_post = self.beta_prior
        
        # Transaction history for user modeling
        self.user_models = {}
    
    def update_global_fraud_rate(self, is_fraud_batch): """Update global fraud rate using Beta-Binomial conjugate prior."""
        frauds = np.sum(is_fraud_batch)
        total = len(is_fraud_batch)
        
        self.alpha_post += frauds
        self.beta_post += (total - frauds)
        
        return self.get_fraud_rate_posterior()
    
    def get_fraud_rate_posterior(self, n_samples=1000): """Sample from posterior distribution of fraud rate."""
        samples = np.random.beta(self.alpha_post, self.beta_post, n_samples)
        return {
            'mean': self.alpha_post / (self.alpha_post + self.beta_post),
            'samples': samples,
            'credible_interval': np.percentile(samples,
    [
        2.5,
        97.5
    ])
}
    
    def update_user_model(self, user_id, transaction_features, is_fraud): """Update user-specific transaction model."""
        if user_id not in self.user_models:
            self.user_models[user_id
] = {
                'transaction_count': 0,
                'fraud_count': 0,
                'amount_history': [],
                'category_history': [],
                'region_history': []
}
        
        model = self.user_models[user_id
]
        model['transaction_count'
] += 1
        model['fraud_count'
] += int(is_fraud)
        model['amount_history'
].append(transaction_features['amount'
])
        model['category_history'
].append(transaction_features['merchant_category'
])
        model['region_history'
].append(transaction_features['region'
])
        
        # Keep only recent history (sliding window)
        window_size = 100
        for key in ['amount_history', 'category_history', 'region_history'
]:
            if len(model[key
]) > window_size:
                model[key
] = model[key
][-window_size:
]
    
    def calculate_anomaly_score(self, user_id, transaction_features): """Calculate anomaly score for a transaction."""
        if user_id not in self.user_models:
            return 0.5  # Neutral score for new users
        
        model = self.user_models[user_id
]
        if model['transaction_count'
] < 5:
            return 0.3  # Low score for users with little history
        
        score = 0.0
        
        # Amount anomaly (using z-score)
        if len(model['amount_history'
]) > 1:
            mean_amount = np.mean(model['amount_history'
])
            std_amount = np.std(model['amount_history'
])
            if std_amount > 0:
                z_score = abs(transaction_features['amount'
] - mean_amount) / std_amount
                score += min(z_score / 3,
0.4)  # Cap at 0.4
        
        # Category anomaly
        category_freq = {}
        for cat in model['category_history'
]:
            category_freq[cat
] = category_freq.get(cat,
0) + 1
        
        total_cats = len(model['category_history'
])
        current_cat_freq = category_freq.get(transaction_features['merchant_category'
],
0)
        category_prob = current_cat_freq / total_cats if total_cats > 0 else 0.1
        score += (1 - category_prob) * 0.3
        
        # Region anomaly
        region_freq = {}
        for reg in model['region_history'
]:
            region_freq[reg
] = region_freq.get(reg,
0) + 1
        
        total_regions = len(model['region_history'
])
        current_region_freq = region_freq.get(transaction_features['region'
],
0)
        region_prob = current_region_freq / total_regions if total_regions > 0 else 0.2
        score += (1 - region_prob) * 0.3
        
        return min(score,
1.0)

# Initialize and train the Bayesian detector
print("Training Bayesian Fraud Detector...")
detector = BayesianFraudDetector()

# Split data for training and testing
train_data, test_data = train_test_split(transactions, test_size=0.3, random_state=42, 
                                        stratify=transactions['is_fraud'
])

print(f"Training set: {len(train_data):,} transactions")
print(f"Test set: {len(test_data):,} transactions")

# Train on historical data
for idx, row in train_data.iterrows():
    transaction_features = {
        'amount': row['amount'
    ],
        'merchant_category': row['merchant_category'
    ],
        'region': row['region'
    ]
}
    detector.update_user_model(row['user_id'
], transaction_features, row['is_fraud'
])

# Update global fraud rate
fraud_rate_posterior = detector.update_global_fraud_rate(train_data['is_fraud'
])

print(f"Training completed")
print(f"Estimated fraud rate: {fraud_rate_posterior['mean']:.3f}")
print(f"95% Credible interval: [{fraud_rate_posterior['credible_interval'][0]:.3f}, {fraud_rate_posterior['credible_interval'][1]:.3f}]")
print(f"User models created: {len(detector.user_models):,}")





# Visualize fraud rate posterior
fig = make_subplots(
    rows=1, cols=2,
    subplot_titles=('Fraud Rate Posterior Distribution', 'Bayesian Updating Process')
)

# Posterior distribution
samples = fraud_rate_posterior['samples'
]
fig.add_trace(
    go.Histogram(x=samples, nbinsx=50, name='Posterior', opacity=0.7),
    row=1, col=1
)

# Add credible interval lines
ci = fraud_rate_posterior['credible_interval'
]
fig.add_vline(x=ci[
    0
], line_dash="dash", line_color="red", 
              annotation_text=f"2.5%: {ci[0]:.3f}", row=1, col=1)
fig.add_vline(x=ci[
    1
], line_dash="dash", line_color="red", 
              annotation_text=f"97.5%: {ci[1]:.3f}", row=1, col=1)

# Simulate Bayesian updating process
batch_size = 1000
n_batches = len(train_data) // batch_size
updating_process = []

temp_detector = BayesianFraudDetector()
for i in range(n_batches):
    start_idx = i * batch_size
    end_idx = (i + 1) * batch_size
    batch = train_data.iloc[start_idx:end_idx
]
    
    posterior = temp_detector.update_global_fraud_rate(batch['is_fraud'
])
    updating_process.append({
        'batch': i + 1,
        'mean': posterior['mean'
    ],
        'lower_ci': posterior['credible_interval'
    ][
        0
    ],
        'upper_ci': posterior['credible_interval'
    ][
        1
    ]
})

updating_df = pd.DataFrame(updating_process)

# Plot updating process
fig.add_trace(
    go.Scatter(x=updating_df['batch'
], y=updating_df['mean'
], 
              mode='lines+markers', name='Posterior Mean'),
    row=1, col=2
)
fig.add_trace(
    go.Scatter(x=updating_df['batch'
], y=updating_df['upper_ci'
], 
              mode='lines', line=dict(dash='dash'), name='95% CI Upper'),
    row=1, col=2
)
fig.add_trace(
    go.Scatter(x=updating_df['batch'
], y=updating_df['lower_ci'
], 
              mode='lines', line=dict(dash='dash'), name='95% CI Lower'),
    row=1, col=2
)

fig.update_layout(height=500, title_text="Bayesian Fraud Rate Inference")
fig.show()

print("Bayesian inference visualization completed")





class HiddenMarkovFraudModel: """Hidden Markov Model for fraud detection based on transaction sequences."""
    
    def __init__(self, n_states=3):
        self.n_states = n_states
        self.state_names = ['Normal', 'Suspicious', 'Fraud'
]
        
        # Initialize transition matrix (states: Normal, Suspicious, Fraud)
        self.transition_matrix = np.array([
    [
        0.85,
        0.12,
        0.03
    ],  # From Normal
            [
        0.40,
        0.45,
        0.15
    ],  # From Suspicious
            [
        0.10,
        0.30,
        0.60
    ]   # From Fraud
])
        
        # Initial state probabilities
        self.initial_probs = np.array([
    0.90,
    0.08,
    0.02
])
        
        # Emission probabilities for different features
        self.emission_params = {
            'amount': {
                'Normal': {'mean': 50, 'std': 30
        },
                'Suspicious': {'mean': 200, 'std': 100
        },
                'Fraud': {'mean': 1000, 'std': 500
        }
    },
            'hour': {
                'Normal': {'mean': 14, 'std': 4
        },
                'Suspicious': {'mean': 20, 'std': 3
        },
                'Fraud': {'mean': 2, 'std': 2
        }
    }
}
    
    def emission_probability(self, observation, state): """Calculate emission probability for an observation given a state."""
        prob = 1.0
        
        # Amount emission probability
        amount_params = self.emission_params['amount'
][state
]
        amount_prob = stats.norm.pdf(observation['amount'
], 
                                    amount_params['mean'
], 
                                    amount_params['std'
])
        prob *= amount_prob
        
        # Hour emission probability
        hour_params = self.emission_params['hour'
][state
]
        hour_prob = stats.norm.pdf(observation['hour'
], 
                                  hour_params['mean'
], 
                                  hour_params['std'
])
        prob *= hour_prob
        
        return prob
    
    def viterbi_decode(self, observations): """Find most likely state sequence using Viterbi algorithm."""
        n_obs = len(observations)
        
        # Initialize Viterbi tables
        viterbi_prob = np.zeros((n_obs, self.n_states))
        viterbi_path = np.zeros((n_obs, self.n_states), dtype=int)
        
        # Initialize first observation
        for state in range(self.n_states):
            emission_prob = self.emission_probability(observations[
    0
], self.state_names[state
])
            viterbi_prob[
    0, state
] = self.initial_probs[state
] * emission_prob
        
        # Forward pass
        for t in range(1, n_obs):
            for state in range(self.n_states):
                emission_prob = self.emission_probability(observations[t
], self.state_names[state
])
                
                # Find best previous state
                trans_probs = viterbi_prob[t-1
] * self.transition_matrix[
    : , state
]
                best_prev_state = np.argmax(trans_probs)
                
                viterbi_prob[t, state
] = trans_probs[best_prev_state
] * emission_prob
                viterbi_path[t, state
] = best_prev_state
        
        # Backward pass - find best path
        path = np.zeros(n_obs, dtype=int)
        path[
    -1
] = np.argmax(viterbi_prob[
    -1
])
        
        for t in range(n_obs - 2,
-1,
-1):
            path[t
] = viterbi_path[t + 1, path[t + 1
    ]
]
        
        return path, viterbi_prob
    
    def calculate_fraud_probability(self, observations): """Calculate probability of fraud for a sequence of observations."""
        path, probs = self.viterbi_decode(observations)
        
        # Calculate fraud probability as proportion of fraud states
        fraud_states = (path == 2).sum()  # State 2 is Fraud
        suspicious_states = (path == 1).sum()  # State 1 is Suspicious
        
        fraud_prob = fraud_states / len(path)
        suspicious_prob = suspicious_states / len(path)
        
        return {
            'fraud_probability': fraud_prob,
            'suspicious_probability': suspicious_prob,
            'state_sequence': [self.state_names[s
        ] for s in path
    ],
            'most_likely_states': path
}

# Initialize HMM model
print("Initializing Hidden Markov Model...")
hmm_model = HiddenMarkovFraudModel()

# Test HMM on sample user sequences
sample_users = test_data['user_id'
].unique()[
    : 5
]
hmm_results = []

for user_id in sample_users:
    user_transactions = test_data[test_data['user_id'
    ] == user_id
].sort_values('timestamp')
    
    if len(user_transactions) >= 3:  # Need at least 3 transactions
        observations = []
        for _, txn in user_transactions.iterrows():
            observations.append({
                'amount': txn['amount'
    ],
                'hour': txn['hour'
    ]
})
        
        result = hmm_model.calculate_fraud_probability(observations)
        result['user_id'
] = user_id
        result['actual_fraud_rate'
] = user_transactions['is_fraud'
].mean()
        result['n_transactions'
] = len(user_transactions)
        hmm_results.append(result)

print(f"HMM analysis completed for {len(hmm_results)} users")

# Display results
for result in hmm_results[
    : 3
]:
    print(f"\nUser {result['user_id']}:")
    print(f"   Fraud Probability: {result['fraud_probability']:.3f}")
    print(f"   Suspicious Probability: {result['suspicious_probability']:.3f}")
    print(f"   Actual Fraud Rate: {result['actual_fraud_rate']:.3f}")
    print(f"   State Sequence: {' → '.join(result['state_sequence'][:10])}...")





class MonteCarloRiskAssessment: """Monte Carlo simulation for fraud risk assessment and optimization."""
    
    def __init__(self, cost_params=None):
        # Default cost parameters
        self.cost_params = cost_params or {
            'fraud_loss_rate': 1.0,  # 100% loss on fraudulent transactions
            'false_positive_cost': 5.0,  # Cost of blocking legitimate transaction
            'investigation_cost': 25.0,  # Cost of manual review
            'reputation_cost': 100.0  # Cost of missed fraud (reputation damage)
}
    
    def simulate_detection_performance(self, fraud_scores, true_labels, threshold, n_simulations=1000): """Simulate detection performance with uncertainty."""
        results = []
        
        for _ in range(n_simulations):
            # Add noise to scores to simulate uncertainty
            noisy_scores = fraud_scores + np.random.normal(0,
0.05, len(fraud_scores))
            predictions = (noisy_scores > threshold).astype(int)
            
            # Calculate metrics
            tp = np.sum((predictions == 1) & (true_labels == 1))
            fp = np.sum((predictions == 1) & (true_labels == 0))
            tn = np.sum((predictions == 0) & (true_labels == 0))
            fn = np.sum((predictions == 0) & (true_labels == 1))
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            
            results.append({
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'true_positives': tp,
                'false_positives': fp,
                'true_negatives': tn,
                'false_negatives': fn
})
        
        return pd.DataFrame(results)
    
    def calculate_expected_cost(self, performance_metrics, transaction_amounts): """Calculate expected cost based on performance metrics."""
        costs = []
        
        for _, metrics in performance_metrics.iterrows():
            # Sample transaction amounts for cost calculation
            fraud_amounts = np.random.choice(transaction_amounts, metrics['true_positives'
] + metrics['false_negatives'
])
            legitimate_amounts = np.random.choice(transaction_amounts, metrics['true_negatives'
] + metrics['false_positives'
])
            
            # Calculate costs
            fraud_loss = np.sum(fraud_amounts[
    :metrics['false_negatives'
    ]
]) * self.cost_params['fraud_loss_rate'
]
            false_positive_cost = metrics['false_positives'
] * self.cost_params['false_positive_cost'
]
            investigation_cost = metrics['true_positives'
] * self.cost_params['investigation_cost'
]
            reputation_cost = metrics['false_negatives'
] * self.cost_params['reputation_cost'
]
            
            total_cost = fraud_loss + false_positive_cost + investigation_cost + reputation_cost
            costs.append(total_cost)
        
        return np.array(costs)
    
    def optimize_threshold(self, fraud_scores, true_labels, transaction_amounts, threshold_range=(0.1,
0.9)): """Find optimal threshold that minimizes expected cost."""
        thresholds = np.linspace(threshold_range[
    0
], threshold_range[
    1
],
20)
        threshold_results = []
        
        for threshold in thresholds:
            performance = self.simulate_detection_performance(fraud_scores, true_labels, threshold, n_simulations=100)
            costs = self.calculate_expected_cost(performance, transaction_amounts)
            
            threshold_results.append({
                'threshold': threshold,
                'mean_cost': np.mean(costs),
                'std_cost': np.std(costs),
                'mean_precision': performance['precision'
    ].mean(),
                'mean_recall': performance['recall'
    ].mean(),
                'mean_f1': performance['f1_score'
    ].mean()
})
        
        results_df = pd.DataFrame(threshold_results)
        optimal_idx = results_df['mean_cost'
].idxmin()
        
        return results_df, results_df.iloc[optimal_idx
]

# Generate fraud scores for test data using our Bayesian detector
print("Running Monte Carlo Risk Assessment...")
test_scores = []

for _, row in test_data.iterrows():
    transaction_features = {
        'amount': row['amount'
    ],
        'merchant_category': row['merchant_category'
    ],
        'region': row['region'
    ]
}
    score = detector.calculate_anomaly_score(row['user_id'
], transaction_features)
    test_scores.append(score)

test_scores = np.array(test_scores)

# Initialize Monte Carlo risk assessment
mc_risk = MonteCarloRiskAssessment()

# Optimize threshold
threshold_results, optimal_threshold = mc_risk.optimize_threshold(
    test_scores, test_data['is_fraud'
].values, test_data['amount'
].values
)

print(f"Monte Carlo optimization completed")
print(f"Optimal threshold: {optimal_threshold['threshold']:.3f}")
print(f"Expected cost: ${optimal_threshold['mean_cost']:,.2f}")
print(f"Precision: {optimal_threshold['mean_precision']:.3f}")
print(f"Recall: {optimal_threshold['mean_recall']:.3f}")
print(f"F1-Score: {optimal_threshold['mean_f1']:.3f}")





# Prepare features for GMM
feature_columns = ['log_amount', 'hour', 'day_of_week', 'user_age', 'account_age_days'
]
X_train = train_data[feature_columns
].values
X_test = test_data[feature_columns
].values

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Fit Gaussian Mixture Model
print("Training Gaussian Mixture Model...")
n_components = 5  # Number of Gaussian components
gmm = GaussianMixture(n_components=n_components, random_state=42, covariance_type='full')
gmm.fit(X_train_scaled)

# Calculate anomaly scores (negative log-likelihood)
train_log_likelihood = gmm.score_samples(X_train_scaled)
test_log_likelihood = gmm.score_samples(X_test_scaled)

# Convert to anomaly scores (higher = more anomalous)
train_anomaly_scores = -train_log_likelihood
test_anomaly_scores = -test_log_likelihood

# Normalize scores to [
    0,
    1
]
min_score = min(train_anomaly_scores.min(), test_anomaly_scores.min())
max_score = max(train_anomaly_scores.max(), test_anomaly_scores.max())

train_anomaly_scores_norm = (train_anomaly_scores - min_score) / (max_score - min_score)
test_anomaly_scores_norm = (test_anomaly_scores - min_score) / (max_score - min_score)

print(f"GMM training completed")
print(f"Number of components: {n_components}")
print(f"Train anomaly score range: [{train_anomaly_scores_norm.min():.3f}, {train_anomaly_scores_norm.max():.3f}]")
print(f"Test anomaly score range: [{test_anomaly_scores_norm.min():.3f}, {test_anomaly_scores_norm.max():.3f}]")

# Evaluate GMM performance
gmm_auc = roc_auc_score(test_data['is_fraud'
], test_anomaly_scores_norm)
print(f"GMM AUC Score: {gmm_auc:.3f}")





# Combine all model scores
model_scores = pd.DataFrame({
    'bayesian_score': test_scores,
    'gmm_score': test_anomaly_scores_norm,
    'is_fraud': test_data['is_fraud'
    ].values,
    'amount': test_data['amount'
    ].values
})

# Create ensemble score (weighted combination)
model_scores['ensemble_score'
] = (0.6 * model_scores['bayesian_score'
] + 
                                 0.4 * model_scores['gmm_score'
])

# Calculate performance metrics for all models
models = ['bayesian_score', 'gmm_score', 'ensemble_score'
]
performance_results = []

for model in models:
    auc = roc_auc_score(model_scores['is_fraud'
], model_scores[model
])
    
    # Find optimal threshold using Youden's J statistic
    fpr, tpr, thresholds = roc_curve(model_scores['is_fraud'
], model_scores[model
])
    optimal_idx = np.argmax(tpr - fpr)
    optimal_threshold = thresholds[optimal_idx
]
    
    # Calculate metrics at optimal threshold
    predictions = (model_scores[model
] > optimal_threshold).astype(int)
    tn, fp, fn, tp = confusion_matrix(model_scores['is_fraud'
], predictions).ravel()
    
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    
    performance_results.append({
        'model': model,
        'auc': auc,
        'optimal_threshold': optimal_threshold,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'true_positives': tp,
        'false_positives': fp,
        'true_negatives': tn,
        'false_negatives': fn
})

performance_df = pd.DataFrame(performance_results)

print("Model Performance Comparison")
print("=" * 50)
for _, row in performance_df.iterrows():
    print(f"\n{row['model'].upper()}:")
    print(f"  AUC: {row['auc']:.3f}")
    print(f"  Precision: {row['precision']:.3f}")
    print(f"  Recall: {row['recall']:.3f}")
    print(f"  F1-Score: {row['f1_score']:.3f}")
    print(f"  Optimal Threshold: {row['optimal_threshold']:.3f}")





# Create comprehensive dashboard
fig = make_subplots(
    rows=3, cols=2,
    subplot_titles=(
        'ROC Curves Comparison', 'Score Distributions by Fraud Status',
        'Threshold Optimization', 'Confusion Matrix Heatmap',
        'Financial Impact Analysis', 'Model Performance Metrics'
    ),
    specs=[
    [
        {
            "secondary_y": False
        },
        {
            "secondary_y": False
        }
    ],
    [
        {
            "secondary_y": False
        },
        {
            "type": "heatmap"
        }
    ],
    [
        {
            "secondary_y": False
        },
        {
            "type": "bar"
        }
    ]
]
)

# ROC Curves
colors = ['blue', 'red', 'green'
]
for i, model in enumerate(models):
    fpr, tpr, _ = roc_curve(model_scores['is_fraud'
], model_scores[model
])
    auc = roc_auc_score(model_scores['is_fraud'
], model_scores[model
])
    
    fig.add_trace(
        go.Scatter(x=fpr, y=tpr, mode='lines', 
                  name=f'{model
} (AUC={auc:.3f
})', 
                  line=dict(color=colors[i
])),
        row=1, col=1
    )

# Add diagonal line for ROC
fig.add_trace(
    go.Scatter(x=[
    0,
    1
], y=[
    0,
    1
], mode='lines', 
              line=dict(dash='dash', color='gray'), 
              name='Random'),
    row=1, col=1
)

# Score distributions
for fraud_status in [
    0,
    1
]:
    mask = model_scores['is_fraud'
] == fraud_status
    fig.add_trace(
        go.Histogram(x=model_scores[mask
]['ensemble_score'
], 
                    name=f'Fraud: {bool(fraud_status)
}', 
                    opacity=0.7, nbinsx=30),
        row=1, col=2
    )

# Threshold optimization curve
fig.add_trace(
    go.Scatter(x=threshold_results['threshold'
], 
              y=threshold_results['mean_cost'
],
              mode='lines+markers', 
              name='Expected Cost'),
    row=2, col=1
)

# Confusion matrix for best model (ensemble)
best_model = performance_df.loc[performance_df['auc'
    ].idxmax()
]
cm_data = [
    [best_model['true_negatives'
        ], best_model['false_positives'
        ]
    ],
    [best_model['false_negatives'
        ], best_model['true_positives'
        ]
    ]
]

fig.add_trace(
    go.Heatmap(z=cm_data, 
              x=['Predicted Normal', 'Predicted Fraud'
],
              y=['Actual Normal', 'Actual Fraud'
],
              colorscale='Blues',
              text=cm_data,
              texttemplate="%{text}",
              textfont={
    "size": 16
}),
    row=2, col=2
)

# Financial impact analysis
fraud_amounts = model_scores[model_scores['is_fraud'
    ] == 1
]['amount'
]
detected_fraud = model_scores[
    (model_scores['is_fraud'
    ] == 1) & 
    (model_scores['ensemble_score'
    ] > best_model['optimal_threshold'
    ])
]['amount'
]

total_fraud_value = fraud_amounts.sum()
detected_fraud_value = detected_fraud.sum()
prevented_loss = detected_fraud_value
missed_fraud_value = total_fraud_value - detected_fraud_value

fig.add_trace(
    go.Bar(x=['Total Fraud', 'Detected', 'Missed'
], 
          y=[total_fraud_value, detected_fraud_value, missed_fraud_value
],
          name='Financial Impact'),
    row=3, col=1
)

# Model performance metrics
fig.add_trace(
    go.Bar(x=performance_df['model'
], 
          y=performance_df['f1_score'
],
          name='F1-Score'),
    row=3, col=2
)

fig.update_layout(height=1200, title_text="Probabilistic Fraud Detection Dashboard")
fig.show()

# Print financial impact summary
print("\nFinancial Impact Analysis")
print("=" * 40)
print(f"Total Fraud Value: ${total_fraud_value:,.2f}")
print(f"Detected Fraud Value: ${detected_fraud_value:,.2f}")
print(f"Prevention Rate: {(detected_fraud_value/total_fraud_value)*100:.1f}%")
print(f"Missed Fraud Value: ${missed_fraud_value:,.2f}")
print(f"\nBest Model: {best_model['model']} (AUC: {best_model['auc']:.3f})")





# Business impact calculation
def calculate_business_impact(performance_metrics, transaction_data, cost_params): """Calculate comprehensive business impact metrics."""
    
    # Current fraud losses (without detection system)
    total_fraud_transactions = transaction_data[transaction_data['is_fraud'
    ] == True
]
    baseline_fraud_loss = total_fraud_transactions['amount'
].sum()
    
    # With detection system
    tp = performance_metrics['true_positives'
]
    fp = performance_metrics['false_positives'
]
    fn = performance_metrics['false_negatives'
]
    tn = performance_metrics['true_negatives'
]
    
    # Estimate prevented fraud value
    avg_fraud_amount = total_fraud_transactions['amount'
].mean()
    prevented_fraud_value = tp * avg_fraud_amount
    missed_fraud_value = fn * avg_fraud_amount
    
    # Calculate costs
    investigation_costs = tp * cost_params['investigation_cost'
]
    false_positive_costs = fp * cost_params['false_positive_cost'
]
    reputation_costs = fn * cost_params['reputation_cost'
]
    
    # Net benefit calculation
    total_costs = investigation_costs + false_positive_costs + reputation_costs
    net_benefit = prevented_fraud_value - total_costs
    roi = (net_benefit / total_costs) * 100 if total_costs > 0 else 0
    
    return {
        'baseline_fraud_loss': baseline_fraud_loss,
        'prevented_fraud_value': prevented_fraud_value,
        'missed_fraud_value': missed_fraud_value,
        'investigation_costs': investigation_costs,
        'false_positive_costs': false_positive_costs,
        'reputation_costs': reputation_costs,
        'total_costs': total_costs,
        'net_benefit': net_benefit,
        'roi_percentage': roi,
        'fraud_prevention_rate': (prevented_fraud_value / baseline_fraud_loss) * 100
}

# Calculate business impact for best model
best_model_metrics = performance_df.loc[performance_df['auc'
    ].idxmax()
]
business_impact = calculate_business_impact(
    best_model_metrics, 
    test_data, 
    mc_risk.cost_params
)

print("BUSINESS IMPACT ANALYSIS")
print("=" * 50)
print(f"\nFraud Prevention:")
print(f"   Baseline Fraud Loss: ${business_impact['baseline_fraud_loss']:,.2f}")
print(f"   Prevented Fraud: ${business_impact['prevented_fraud_value']:,.2f}")
print(f"   Prevention Rate: {business_impact['fraud_prevention_rate']:.1f}%")

print(f"\nCost Analysis:")
print(f"   Investigation Costs: ${business_impact['investigation_costs']:,.2f}")
print(f"   False Positive Costs: ${business_impact['false_positive_costs']:,.2f}")
print(f"   Reputation Costs: ${business_impact['reputation_costs']:,.2f}")
print(f"   Total Operational Costs: ${business_impact['total_costs']:,.2f}")

print(f"\nBottom Line:")
print(f"   Net Benefit: ${business_impact['net_benefit']:,.2f}")
print(f"   ROI: {business_impact['roi_percentage']:.1f}%")

# Risk assessment by transaction segments
print(f"\nRisk Assessment by Segments:")
segments = {
    'High Value (>$500)': test_data['amount'
    ] > 500,
    'Online Transactions': test_data['merchant_category'
    ] == 'online',
    'Night Transactions': test_data['is_night'
    ] == 1,
    'Weekend Transactions': test_data['is_weekend'
    ] == 1
}

for segment_name, mask in segments.items():
    segment_data = test_data[mask
]
    if len(segment_data) > 0:
        fraud_rate = segment_data['is_fraud'
].mean()
        avg_amount = segment_data['amount'
].mean()
        print(f"   {segment_name}:")
        print(f"     Fraud Rate: {fraud_rate:.2%}")
        print(f"     Avg Amount: ${avg_amount:.2f}")
        print(f"     Volume: {len(segment_data):,} transactions")





# Final summary statistics
print("PROJECT SUMMARY")
print("=" * 50)
print(f"\nDataset Statistics:")
print(f"   Total Transactions: {len(transactions):,}")
print(f"   Unique Users: {transactions['user_id'].nunique():,}")
print(f"   Fraud Rate: {transactions['is_fraud'].mean():.2%}")
print(f"   Total Volume: ${transactions['amount'].sum():,.2f}")

print(f"\nModel Performance:")
print(f"   Best Model: {best_model_metrics['model']}")
print(f"   AUC Score: {best_model_metrics['auc']:.3f}")
print(f"   Precision: {best_model_metrics['precision']:.3f}")
print(f"   Recall: {best_model_metrics['recall']:.3f}")
print(f"   F1-Score: {best_model_metrics['f1_score']:.3f}")

print(f"\nBusiness Impact:")
print(f"   Fraud Prevention Rate: {business_impact['fraud_prevention_rate']:.1f}%")
print(f"   Net Benefit: ${business_impact['net_benefit']:,.2f}")
print(f"   ROI: {business_impact['roi_percentage']:.1f}%")

print(f"\nProbability Methods Applied:")
print(f"   Bayesian Inference (Beta-Binomial)")
print(f"   Hidden Markov Models")
print(f"   Monte Carlo Simulation")
print(f"   Gaussian Mixture Models")
print(f"   Bayesian Decision Theory")

print(f"\nReady for Production Deployment!")
print(f"\nThis notebook demonstrates:")
print(f"   • Advanced probability theory in practice")
print(f"   • Real-world data science applications")
print(f"   • Business-driven model optimization")
print(f"   • Uncertainty quantification")
print(f"   • Cost-sensitive machine learning")

# Save results for future use
results_summary = {
    'model_performance': performance_df.to_dict('records'),
    'business_impact': business_impact,
    'optimal_threshold': optimal_threshold.to_dict(),
    'fraud_rate_posterior': fraud_rate_posterior
}

print(f"\nResults saved for future analysis")
print(f"\nProbabilistic Fraud Detection System Complete!")
